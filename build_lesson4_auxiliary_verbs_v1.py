import argparse
import json
# -*- coding: utf-8 -*-
# build_lesson4_auxiliary_verbs_v1.py
# –ì–µ–Ω–µ—Ä–∏—Ç DOCX: cha_lesson_4_auxiliary_verbs_v1.docx –Ω–∞ –æ—Å–Ω–æ–≤–µ cha_lesson_4_auxiliary_verbs_lite_v3.docx
# –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
# - –î–æ–±–∞–≤–∏—Ç—å RU —Å—Ç—Ä–æ–∫—É –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π EN —Å—Ç—Ä–æ–∫–∏ –≤ Explanation / Practice / Vocabulary Exercises / Exit check & Homework
# - –í Vocabulary –ø–æ—Å–ª–µ RU –¥–æ–±–∞–≤–∏—Ç—å ¬´ ‚Äî TH¬ª –ø–µ—Ä–µ–≤–æ–¥ –º–æ–¥–∞–ª—å–Ω—ã—Ö/–≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö.
import os
import re
import time

from docx import Document
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from docx.shared import Pt, RGBColor, Cm

# ---------- –¶–≤–µ—Ç–∞ –∏ —Å—Ç–∏–ª–∏ ----------
GOLD = RGBColor(184, 134, 11)
BLACK = RGBColor(0, 0, 0)
DARK_RED = RGBColor(139, 0, 0)
DARK_GREEN = RGBColor(0, 100, 0)
PURPLE = RGBColor(102, 0, 153)

THAI_FONT_NAME = "Noto Sans Thai"

SRC_NAME = "cha_lesson_4_auxiliary_verbs_lite_v3.docx"
OUT_NAME = "cha_lesson_4_auxiliary_verbs_v1.docx"

# –¢–∞–π—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è Vocabulary (–∫–ª—é—á ‚Äî –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π EN —Ç–µ—Ä–º–∏–Ω)
TH_VOCAB = {
    "can": "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ",
    "could": "‡∏≠‡∏≤‡∏à‡∏à‡∏∞ / ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ(‡∏≠‡∏î‡∏µ‡∏ï)",
    "may": "‡∏≠‡∏≤‡∏à‡∏à‡∏∞",
    "might": "‡∏≠‡∏≤‡∏à‡∏à‡∏∞",
    "must": "‡∏ï‡πâ‡∏≠‡∏á",
    "have to": "‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á / ‡∏ï‡πâ‡∏≠‡∏á",
    "has to": "‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á / ‡∏ï‡πâ‡∏≠‡∏á",
    "had to": "‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á / ‡∏ï‡πâ‡∏≠‡∏á (‡∏≠‡∏î‡∏µ‡∏ï)",
    "should": "‡∏Ñ‡∏ß‡∏£",
    "would": "‡∏à‡∏∞ / ‡∏°‡∏±‡∏Å‡∏à‡∏∞ (‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥)",
    "will": "‡∏à‡∏∞",
    "shall": "‡∏à‡∏∞ (‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£)",
    "do": "‡∏ó‡∏≥ (‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏ß‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå)",
    "does": "‡∏ó‡∏≥ (‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏ß‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå)",
    "did": "‡∏ó‡∏≥ (‡∏≠‡∏î‡∏µ‡∏ï, ‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏ß‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå)",
    "be": "‡πÄ‡∏õ‡πá‡∏ô/‡∏≠‡∏¢‡∏π‡πà/‡∏Ñ‡∏∑‡∏≠",
    "am": "‡πÄ‡∏õ‡πá‡∏ô/‡∏≠‡∏¢‡∏π‡πà/‡∏Ñ‡∏∑‡∏≠",
    "is": "‡πÄ‡∏õ‡πá‡∏ô/‡∏≠‡∏¢‡∏π‡πà/‡∏Ñ‡∏∑‡∏≠",
    "are": "‡πÄ‡∏õ‡πá‡∏ô/‡∏≠‡∏¢‡∏π‡πà/‡∏Ñ‡∏∑‡∏≠",
    "was": "‡πÄ‡∏õ‡πá‡∏ô/‡∏≠‡∏¢‡∏π‡πà/‡∏Ñ‡∏∑‡∏≠",
    "were": "‡πÄ‡∏õ‡πá‡∏ô/‡∏≠‡∏¢‡∏π‡πà/‡∏Ñ‡∏∑‡∏≠",
    "have": "‡∏°‡∏µ / ‡πÑ‡∏î‡πâ‡∏ó‡∏≥ (‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå)",
    "has": "‡∏°‡∏µ / ‡πÑ‡∏î‡πâ‡∏ó‡∏≥ (‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå)",
    "had": "‡∏°‡∏µ / ‡πÑ‡∏î‡πâ‡∏ó‡∏≥ (‡∏≠‡∏î‡∏µ‡∏ï)",
    "be able to": "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ",
    "need to": "‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á",
    "ought to": "‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞",
    "used to": "‡πÄ‡∏Ñ‡∏¢",
    "dare": "‡∏Å‡∏•‡πâ‡∏≤",
    "had better": "‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞...‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤",
}

# –¢–∞–π—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è Word bank (School & Stationery)
WORD_BANK_TH = {
    "notebook": "‡∏™‡∏°‡∏∏‡∏î‡πÇ‡∏ô‡πâ‡∏ï",
    "textbook": "‡∏ï‡∏≥‡∏£‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô",
    "workbook": "‡∏™‡∏°‡∏∏‡∏î‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î",
    "binder": "‡πÅ‡∏ü‡πâ‡∏°‡∏™‡∏±‡∏ô‡∏´‡πà‡∏ß‡∏á",
    "folder": "‡πÅ‡∏ü‡πâ‡∏°",
    "loose-leaf paper": "‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏©‡πÅ‡∏¢‡∏Å‡πÅ‡∏ú‡πà‡∏ô",
    "pen": "‡∏õ‡∏≤‡∏Å‡∏Å‡∏≤",
    "pencil": "‡∏î‡∏¥‡∏ô‡∏™‡∏≠",
    "eraser": "‡∏¢‡∏≤‡∏á‡∏•‡∏ö",
    "sharpener": "‡∏Å‡∏ö‡πÄ‡∏´‡∏•‡∏≤‡∏î‡∏¥‡∏ô‡∏™‡∏≠",
    "highlighter": "‡∏õ‡∏≤‡∏Å‡∏Å‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°",
    "marker": "‡∏õ‡∏≤‡∏Å‡∏Å‡∏≤‡πÄ‡∏°‡∏à‡∏¥‡∏Å",
    "ruler": "‡πÑ‡∏°‡πâ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î",
    "protractor": "‡πÑ‡∏°‡πâ‡πÇ‡∏õ‡∏£‡πÅ‡∏ó‡∏£‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå",
    "compass (geometry)": "‡∏ß‡∏á‡πÄ‡∏ß‡∏µ‡∏¢‡∏ô",
    "glue stick": "‡∏Å‡∏≤‡∏ß‡πÅ‡∏ó‡πà‡∏á",
    "scissors": "‡∏Å‡∏£‡∏£‡πÑ‡∏Å‡∏£",
    "stapler": "‡∏ó‡∏µ‡πà‡πÄ‡∏¢‡πá‡∏ö‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏©",
    "paper clips": "‡∏Ñ‡∏•‡∏¥‡∏õ‡∏´‡∏ô‡∏µ‡∏ö‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏©",
    "sticky notes": "‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏©‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏≠‡∏¥‡∏ó",
}

# –ë–∞–∑–æ–≤—ã–π RU-—Å–ª–æ–≤–∞—Ä—å –¥–ª—è Vocabulary
RU_VOCAB = {
    "can": "–º–æ–∂–µ—Ç",
    "could": "–º–æ–≥(–ª–∞)/–º–æ–≥–ª–∏",
    "may": "–º–æ–∂–µ—Ç",
    "might": "–≤–æ–∑–º–æ–∂–Ω–æ",
    "must": "–¥–æ–ª–∂–µ–Ω",
    "have to": "–¥–æ–ª–∂–µ–Ω/–ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è",
    "has to": "–¥–æ–ª–∂–µ–Ω/–ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è",
    "had to": "–¥–æ–ª–∂–µ–Ω –±—ã–ª/–ø—Ä–∏—à–ª–æ—Å—å",
    "should": "—Å–ª–µ–¥—É–µ—Ç",
    "would": "–±—ã",
    "will": "–±—É–¥–µ—Ç",
    "shall": "–±—É–¥–µ—Ç (–æ—Ñ–∏—Ü.)",
    "do": "–¥–µ–ª–∞—Ç—å (–≤—Å–ø.)",
    "does": "–¥–µ–ª–∞–µ—Ç (–≤—Å–ø.)",
    "did": "—Å–¥–µ–ª–∞–ª (–≤—Å–ø.)",
    "be": "–±—ã—Ç—å",
    "am": "–µ—Å—Ç—å",
    "is": "–µ—Å—Ç—å",
    "are": "–µ—Å—Ç—å",
    "was": "–±—ã–ª",
    "were": "–±—ã–ª–∏",
    "have": "–∏–º–µ—Ç—å",
    "has": "–∏–º–µ–µ—Ç",
    "had": "–∏–º–µ–ª",
    "be able to": "–º–æ–∂–µ—Ç/–≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏",
    "need to": "–Ω—É–∂–Ω–æ/–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ",
    "ought to": "—Å–ª–µ–¥–æ–≤–∞–ª–æ –±—ã",
    "used to": "—Ä–∞–Ω—å—à–µ –¥–µ–ª–∞–ª/–æ–±—ã—á–Ω–æ",
    "dare": "–æ—Å–º–µ–ª–∏–≤–∞—Ç—å—Å—è",
    "had better": "–ª—É—á—à–µ –±—ã/—Å–ª–µ–¥—É–µ—Ç",
}


# (–∞–≤—Ç–æ–ø–µ—Ä–µ–≤–æ–¥ —É–¥–∞–ª—ë–Ω; –ø–µ—Ä–µ–≤–æ–¥—ã –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞)


def new_doc():
    doc = Document()
    for s in doc.sections:
        s.page_height = Cm(29.7)
        s.page_width = Cm(21.0)
        s.left_margin = Cm(2.0)
        s.right_margin = Cm(2.0)
        s.top_margin = Cm(2.0)
        s.bottom_margin = Cm(2.0)
        fp = s.footer.paragraphs[0]
        fp.alignment = WD_ALIGN_PARAGRAPH.CENTER
        run1 = fp.add_run("¬© Cha 2025 ¬∑ Page ")
        run1.font.size = Pt(9)
        run1.font.color.rgb = BLACK
        fld = OxmlElement("w:fldSimple")
        fld.set(qn("w:instr"), "PAGE")
        run2 = fp.add_run()
        run2._r.append(fld)
    return doc


def clone_run(dst_p, src_run):
    r = dst_p.add_run(src_run.text)
    # –±–∞–∑–æ–≤—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
    try:
        r.font.bold = src_run.font.bold
        r.font.italic = src_run.font.italic
        r.font.underline = src_run.font.underline
        r.font.size = src_run.font.size
        r.font.all_caps = src_run.font.all_caps
        if src_run.font.color and src_run.font.color.rgb:
            r.font.color.rgb = src_run.font.color.rgb
    except Exception:
        pass
    return r


def clone_paragraph(dst_doc, src_p):
    p = dst_doc.add_paragraph()
    # –∫–æ–ø–∏—Ä—É–µ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    p.alignment = src_p.alignment
    for run in src_p.runs:
        clone_run(p, run)
    return p


# (—É–¥–∞–ª–µ–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ –∞–≤—Ç–æ–ø–µ—Ä–µ–≤–æ–¥–∞; –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∑–∞—Ä–∞–Ω–µ–µ –∑–∞–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã)


# (—É–¥–∞–ª–µ–Ω—ã –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)


def is_vocab_item(text: str) -> bool:
    # –ü—Ä–∏–º–∏—Ç–∏–≤–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Å—Ç—Ä–æ–∫–∞ —Ç–∏–ø–∞ "A. can ‚Äî –º–æ–∂–µ—Ç"
    t = text.strip()
    if re.match(r"^[A-Za-z]\.\s+", t):
        return True
    # –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å EN ‚Äî RU —É–∂–µ
    if " ‚Äî " in t and not any(ch.isdigit() for ch in t.split(" ‚Äî ")[0][:3]):
        return True
    return False


def normalize_key(s: str) -> str:
    return re.sub(r"\s+", " ", s.strip().lower())


def norm_exact(s: str) -> str:
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–π –¥–ª—è —Å–ª–æ–≤–∞—Ä—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤ (–±–µ–∑ –Ω–∏–∂–Ω–µ–≥–æ —Ä–µ–≥–∏—Å—Ç—Ä–∞)
    s = (s or "").strip()
    # –£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏—Ä–µ/–¥–µ—Ñ–∏—Å–æ–≤ –∏ –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã—Ö –¥–µ—Ñ–∏—Å–æ–≤
    s = s.replace("\u2011", "-")  # non-breaking hyphen
    s = s.replace("\u2013", "-")  # en dash
    s = s.replace("\u2014", "-")  # em dash
    s = s.replace("\u2212", "-")  # minus sign
    # –°—Ä–µ–∑–∞–µ–º –≤–µ–¥—É—â—É—é –Ω—É–º–µ—Ä–∞—Ü–∏—é –≤–∏–¥–∞ 1)  1.  1.1  –∏ —Ç.–ø.
    s = re.sub(r"^\s*\d+(?:\.\d+)*[\)\.]?\s+", "", s)
    # –°—Ä–µ–∑–∞–µ–º –ª–∏–¥–∏—Ä—É—é—â–∏–µ –º–∞—Ä–∫–µ—Ä—ã —Å–ø–∏—Å–∫–æ–≤ (‚Ä¢, -, ‚Äì, ‚Äî) –∏ –ø—Ä–æ–±–µ–ª—ã
    s = re.sub(r"^[\u2022\-\u2013\u2014]\s+", "", s)
    # –£–ø—Ä–æ—â–∞–µ–º –ø—Ä–æ–±–µ–ª—ã
    return re.sub(r"\s+", " ", s)


def strip_list_markers(s: str) -> str:
    return re.sub(r"^[\u2022\-\u2013\u2014]\s+", "", (s or "").strip())


def clean_vocab_en_term(s: str) -> str:
    """–û—á–∏—â–∞–µ—Ç EN-—Ç–µ—Ä–º–∏–Ω –≤ Word bank: —É–±–∏—Ä–∞–µ—Ç –ª–∏—Ç–µ—Ä–Ω—É—é –Ω—É–º–µ—Ä–∞—Ü–∏—é (a.), —ç–º–æ–¥–∑–∏, –æ—Å—Ç–∞–≤–ª—è–µ—Ç –ª–∞—Ç–∏–Ω–∏—Ü—É/–ø—Ä–æ–±–µ–ª—ã/—Å–∫–æ–±–∫–∏/–¥–µ—Ñ–∏—Å."""
    s = (s or "").strip()
    # —É–±—Ä–∞—Ç—å a./b./c.
    s = re.sub(r"^[A-Za-z]\.[\s]+", "", s)
    # —É–±—Ä–∞—Ç—å —ç–º–æ–¥–∑–∏ –∏ –ø—Ä–æ—á–∏–µ —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ –ª–∞—Ç–∏–Ω–∏—Ü—ã, –ø—Ä–æ–±–µ–ª–æ–≤, –¥–µ—Ñ–∏—Å–∞ –∏ ()
    s = re.sub(r"[^A-Za-z()\-\s]", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s.lower()


BLOCK_TITLES = {
    "‚úèÔ∏è Lesson 4 ‚Äî Auxiliary Verbs ‚Äî Vocabulary: School & Stationery",
    "üë©‚Äçüè´ Explanation",
    "üß† Practice",
    "‚úçÔ∏è Examples:",
    "‚úèÔ∏è Vocabulary (School & Stationery)",
    "‚úèÔ∏è Vocabulary",
    "‚úèÔ∏è Vocabulary Exercises",
    "üßæ Exit check (5 quick items):",
    "üßæ Exit check & Homework",
}


def load_translations_json(path: str) -> dict:
    if not path or not os.path.exists(path):
        return {}
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–ª—é—á–∏
    return {norm_exact(k): v for k, v in data.items()}


def load_translations_from_source(path: str) -> dict:
    """
    –ü–∞—Ä—Å–∏—Ç —Ñ–∞–π–ª, –≥–¥–µ EN —Å—Ç—Ä–æ–∫–∞ –∏–¥—ë—Ç –æ—Ç–¥–µ–ª—å–Ω–æ, –∞ –Ω–∏–∂–µ 2 —Å—Ç—Ä–æ–∫–∏ –≤ —Å–∫–æ–±–∫–∞—Ö ‚Äî RU –∏ TH.
    –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –±–æ–ª—å—à–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –±–ª–æ–∫–æ–≤ –∏ —Å–ª–æ–≤–∞—Ä—å Word bank.
    """
    if not path or not os.path.exists(path):
        return {}
    with open(path, "r", encoding="utf-8") as f:
        lines = [ln.rstrip("\n") for ln in f]
    tr = {}
    section = None
    i = 0
    while i < len(lines):
        L = lines[i].strip()
        if not L:
            i += 1
            continue
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–∫—Ü–∏—é –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
        low = L.lower()
        if L in BLOCK_TITLES:
            if "vocabulary" in low and "exercises" not in low:
                section = "vocab"
            elif "vocabulary exercises" in low:
                section = "vocab_ex"
            elif "practice" in low:
                section = "practice"
            elif "exit check" in low or "homework" in low:
                section = "exit"
            elif "explanation" in low:
                section = "expl"
            i += 1
            continue
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à–∞–ø–∫–∏ Word bank –∏ —Å–∞–º–∏ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ª–æ–≤–∞—Ä—è ‚Äî –æ–Ω–∏ –Ω–µ –ø–µ—Ä–µ–≤–æ–¥—è—Ç—Å—è —ç—Ç–∏–º —Å–ª–æ–µ–º
        if section == "vocab":
            i += 1
            continue
        # EN-—Å—Ç—Ä–æ–∫–∞ ‚Äî –µ—Å–ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–∞ '(' ‚Äî —ç—Ç–æ RU, –∞ —Å–ª–µ–¥—É—é—â–∞—è –∑–∞ –Ω–µ–π ‚Äî TH
        if not L.startswith("("):
            ru = th = None
            if i + 1 < len(lines) and lines[i + 1].strip().startswith("("):
                ru = lines[i + 1].strip()
                if ru.startswith("(") and ru.endswith(")"):
                    ru = ru[1:-1]
            if i + 2 < len(lines) and lines[i + 2].strip().startswith("("):
                th = lines[i + 2].strip()
                if th.startswith("(") and th.endswith(")"):
                    th = th[1:-1]
            if ru or th:
                base_key = norm_exact(L)
                val = {"ru": ru, "th": th}
                tr[base_key] = val
                # –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –∫–ª—é—á –±–µ–∑ –º–∞—Ä–∫–µ—Ä–æ–≤ —Å–ø–∏—Å–∫–æ–≤
                alt = norm_exact(strip_list_markers(L))
                if alt != base_key:
                    tr[alt] = val
                i += 3
                continue
        i += 1
    return tr


def collect_highlight_tokens(src_p) -> list:
    """–°–æ–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω—ã (–≤–µ—Ä—Ö–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä) –∏–∑ EN-–∞–±–∑–∞—Ü–∞ –¥–ª—è –∑–µ—Ä–∫–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è –≤ RU."""
    tokens = []
    for run in src_p.runs:
        t = run.text or ""
        # –∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ–º –∫—É—Å–∫–∏ –≤–∏–¥–∞ ALL CAPS (–≤–∫–ª—é—á–∞—è —Ñ—Ä–∞–∑—ã —Å –ø—Ä–æ–±–µ–ª–∞–º–∏)
        for m in re.finditer(r"[A-Z][A-Z ]+[A-Z]", t):
            tok = m.group(0).strip()
            if tok not in tokens:
                tokens.append(tok)
        # –µ—Å–ª–∏ —Ä–∞–Ω –ø–æ–¥—á—ë—Ä–∫–Ω—É—Ç –∏ –±–µ–∑ –∫–∞–ø—Å–∞ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –∑–∞—Ö–≤–∞—Ç–∏—Ç—å —Å–ª–æ–≤–æ
        try:
            if run.font and run.font.underline and not any(
                    ch.isupper() for ch in t):
                # –±–µ—Ä–µ–º –∫–æ—Ä–æ—Ç–∫—É—é –º–µ—Ç–∫—É –¥–æ 15 —Å–∏–º–≤–æ–ª–æ–≤
                w = t.strip()
                if 0 < len(w) <= 15 and w not in tokens:
                    tokens.append(w)
        except Exception:
            pass
    # —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (–¥–ª–∏–Ω–Ω–µ–µ –≤–ø–µ—Ä—ë–¥), —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑–±–∏–≤–∞—Ç—å —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
    tokens.sort(key=len, reverse=True)
    return tokens


def add_ru_mapped_line_with_highlights(doc, src_p, ru_text):
    """–†–∏—Å—É–µ–º RU —Å—Ç—Ä–æ–∫—É –∏–∑ —Å–ª–æ–≤–∞—Ä—è, –Ω–æ –∑–µ—Ä–∫–∞–ª–∏–º –ø–æ–¥—á—ë—Ä–∫–Ω—É—Ç—ã–µ/ALL CAPS —Ç–æ–∫–µ–Ω—ã –∏–∑ EN, –µ—Å–ª–∏ –æ–Ω–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ RU.
    –§–æ–Ω RU ‚Äî —Ç—ë–º–Ω–æ-–∫—Ä–∞—Å–Ω—ã–π –∫—É—Ä—Å–∏–≤; —Å–æ–≤–ø–∞–≤—à–∏–µ —Ç–æ–∫–µ–Ω—ã ‚Äî —á—ë—Ä–Ω—ã–π bold+underline (–∏ –±–µ–∑ –∫—É—Ä—Å–∏–≤–∞).
    """
    p = doc.add_paragraph()
    # –æ—Ç–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∞
    r0 = p.add_run("(")
    r0.font.italic = True
    r0.font.color.rgb = DARK_RED

    hi = collect_highlight_tokens(src_p)
    s = ru_text or ""
    i = 0
    while i < len(s):
        hit_pos = None
        hit_tok = None
        # –∏—â–µ–º –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –ª—é–±–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
        for tok in hi:
            j = s.find(tok, i)
            if j != -1 and (hit_pos is None or j < hit_pos):
                hit_pos = j
                hit_tok = tok
        if hit_pos is None:
            # —Ö–≤–æ—Å—Ç ‚Äî –æ–±—ã—á–Ω—ã–π RU
            r = p.add_run(s[i:])
            r.font.italic = True
            r.font.color.rgb = DARK_RED
            break
        # –ø—Ä–µ–ª—é–¥–∏—è –¥–æ —Ç–æ–∫–µ–Ω–∞
        if hit_pos > i:
            r = p.add_run(s[i:hit_pos])
            r.font.italic = True
            r.font.color.rgb = DARK_RED
        # —Å–∞–º —Ç–æ–∫–µ–Ω ‚Äî —á—ë—Ä–Ω—ã–π bold+underline
        r2 = p.add_run(s[hit_pos:hit_pos + len(hit_tok)])
        r2.font.color.rgb = BLACK
        r2.font.bold = True
        r2.font.underline = True
        r2.font.italic = False
        i = hit_pos + len(hit_tok)

    # –∑–∞–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∞
    rz = p.add_run(")")
    rz.font.italic = True
    rz.font.color.rgb = DARK_RED


def add_th_mapped_line_with_highlights(doc, src_p, th_text):
    """TH —Å—Ç—Ä–æ–∫–∞ —Å –∑–µ—Ä–∫–∞–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–æ–¥—á—ë—Ä–∫–Ω—É—Ç—ã—Ö/ALL CAPS —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ EN.
    –ë–∞–∑–∞ ‚Äî –∑–µ–ª—ë–Ω—ã–π –∫—É—Ä—Å–∏–≤; —Å–æ–≤–ø–∞–≤—à–∏–µ —Ç–æ–∫–µ–Ω—ã ‚Äî —á—ë—Ä–Ω—ã–π bold+underline (–±–µ–∑ –∫—É—Ä—Å–∏–≤–∞).
    """
    p = doc.add_paragraph()
    r0 = p.add_run("(")
    r0.font.italic = True
    r0.font.color.rgb = DARK_GREEN

    hi = collect_highlight_tokens(src_p)
    s = th_text or ""
    i = 0
    while i < len(s):
        hit_pos = None
        hit_tok = None
        for tok in hi:
            j = s.find(tok, i)
            if j != -1 and (hit_pos is None or j < hit_pos):
                hit_pos = j
                hit_tok = tok
        if hit_pos is None:
            r = p.add_run(s[i:])
            r.font.italic = True
            r.font.color.rgb = DARK_GREEN
            r.font.name = THAI_FONT_NAME
            break
        if hit_pos > i:
            r = p.add_run(s[i:hit_pos])
            r.font.italic = True
            r.font.color.rgb = DARK_GREEN
            r.font.name = THAI_FONT_NAME
        r2 = p.add_run(s[hit_pos:hit_pos + len(hit_tok)])
        r2.font.color.rgb = BLACK
        r2.font.bold = True
        r2.font.underline = True
        r2.font.italic = False
        r2.font.name = THAI_FONT_NAME
        i = hit_pos + len(hit_tok)

    rz = p.add_run(")")
    rz.font.italic = True
    rz.font.color.rgb = DARK_GREEN


def append_th_to_vocab_line(dst_p):
    # –†–∞–∑–±–∏—Ä–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–æ–∫—É, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å EN —Ç–µ—Ä–º–∏–Ω
    full = dst_p.text
    parts = full.split(" ‚Äî ")
    # EN —á–∞—Å—Ç—å –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–∏—Ä–µ –∏–ª–∏ –≤—Å—è —Å—Ç—Ä–æ–∫–∞
    en_part = parts[0] if parts else full
    en_term = re.sub(r"^[A-Za-z]\.\s+", "", en_part).strip()

    # –ü–æ–¥–±–æ—Ä –ø–µ—Ä–µ–≤–æ–¥–æ–≤
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º word bank (stationery)
    cleaned = clean_vocab_en_term(en_term)
    th = WORD_BANK_TH.get(cleaned)
    if not th:
        th = TH_VOCAB.get(normalize_key(en_term))
    if not th:
        for k in list(TH_VOCAB.keys()):
            if normalize_key(k) == normalize_key(en_term):
                th = TH_VOCAB[k]
                break
    ru = RU_VOCAB.get(normalize_key(en_term))
    if not ru:
        for k in list(RU_VOCAB.keys()):
            if normalize_key(k) == normalize_key(en_term):
                ru = RU_VOCAB[k]
                break

    # –ï—Å–ª–∏ RU —É–∂–µ –µ—Å—Ç—å –≤ —Å—Ç—Ä–æ–∫–µ ‚Äî –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º TH
    ru_added = False
    th_added = False
    if " ‚Äî " in full:
        if th:
            rr = dst_p.add_run(" ‚Äî ")
            rr.font.italic = True
            rr.font.color.rgb = DARK_GREEN
            tr = dst_p.add_run(th)
            tr.font.italic = True
            tr.font.color.rgb = DARK_GREEN
            tr.font.name = THAI_FONT_NAME
            th_added = True
        else:
            # –ª–æ–≥ –ø—Ä–æ–ø—É—Å–∫–∞ TH –¥–ª—è —Å–ª–æ–≤–∞—Ä–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
            try:
                print(f"[lesson4][miss][Vocab TH] {cleaned or en_term}")
            except Exception:
                pass
        return ru_added, th_added

    # –ï—Å–ª–∏ RU –Ω–µ –±—ã–ª–æ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º RU –∏ TH
    if ru:
        rr_sep = dst_p.add_run(" ‚Äî ")
        rr_sep.font.italic = True
        rr_sep.font.color.rgb = DARK_RED
        rr_run = dst_p.add_run(ru)
        rr_run.font.italic = True
        rr_run.font.color.rgb = DARK_RED
        ru_added = True
    if th:
        th_sep = dst_p.add_run(" ‚Äî ")
        th_sep.font.italic = True
        th_sep.font.color.rgb = DARK_GREEN
        tr = dst_p.add_run(th)
        tr.font.italic = True
        tr.font.color.rgb = DARK_GREEN
        tr.font.name = THAI_FONT_NAME
        th_added = True
    else:
        try:
            print(f"[lesson4][miss][Vocab TH] {cleaned or en_term}")
        except Exception:
            pass
    return ru_added, th_added


def build():
    parser = argparse.ArgumentParser()
    parser.add_argument("--with-ru", dest="with_ru", action="store_true",
                        default=True)
    parser.add_argument("--no-ru", dest="with_ru", action="store_false")
    parser.add_argument("--with-th", dest="with_th", action="store_true",
                        default=True)
    parser.add_argument("--no-th", dest="with_th", action="store_false")
    # –û—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–ª–∞–≥–∏ –¥–ª—è Vocabulary
    parser.add_argument("--vocab-th", dest="vocab_th", action="store_true",
                        default=True)
    parser.add_argument("--no-vocab-th", dest="vocab_th", action="store_false")
    parser.add_argument("--vocab-ru", dest="vocab_ru", action="store_true",
                        default=False)
    parser.add_argument("--no-vocab-ru", dest="vocab_ru", action="store_false")
    parser.add_argument("--translations", type=str,
                        default="lesson4_translations.json")
    parser.add_argument("--translations-source", type=str,
                        default="lesson4_translations_source.txt")
    # (fallback –∞–≤—Ç–æ-–ø–µ—Ä–µ–≤–æ–¥–∞ —É–¥–∞–ª—ë–Ω)
    args = parser.parse_args()
    start_ts = time.time()
    print("[lesson4] Start generation")
    src_path = os.path.join(os.getcwd(), SRC_NAME)
    if not os.path.exists(src_path):
        raise FileNotFoundError(f"Source DOCX not found: {src_path}")
    print(f"[lesson4] Source: {SRC_NAME}")
    src = Document(src_path)

    out = new_doc()
    print("[lesson4] New document initialized")
    # –ì—Ä—É–∑–∏–º –ø–µ—Ä–µ–≤–æ–¥—ã (–∏–∑ source .txt –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ, –∑–∞—Ç–µ–º .json)
    tr_map = {}
    if args.translations_source and os.path.exists(args.translations_source):
        tr_map = load_translations_from_source(args.translations_source)
        print(
            f"[lesson4] Translations loaded from: {args.translations_source} ({len(tr_map)} entries)")
    if not tr_map and args.translations and os.path.exists(args.translations):
        tr_map = load_translations_json(args.translations)
        print(
            f"[lesson4] Translations loaded from: {args.translations} ({len(tr_map)} entries)")

    # –ü—Ä–æ—Å—Ç–∞—è –º–∞—à–∏–Ω–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø–æ —Å–µ–∫—Ü–∏—è–º
    section = None
    ru_lines = 0
    vocab_th_added = 0
    vocab_ru_added = 0

    total = len(src.paragraphs)
    print(f"[lesson4] Paragraphs: {total}")

    for idx, p in enumerate(src.paragraphs, 1):
        text = p.text or ""
        # –ö–ª–æ–Ω–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –µ—Å—Ç—å
        new_p = clone_paragraph(out, p)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–º–µ–Ω—É —Å–µ–∫—Ü–∏–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        t = text.strip().lower()
        if "vocabulary" in t and len(t) < 64:
            if section != "vocab":
                print("[lesson4] --> Section: Vocabulary")
            section = "vocab"
        elif "vocabulary exercises" in t:
            if section != "vocab_ex":
                print("[lesson4] --> Section: Vocabulary Exercises")
            section = "vocab_ex"
        elif "practice" in t:
            if section != "practice":
                print("[lesson4] --> Section: Practice")
            section = "practice"
        elif "exit check" in t or "homework" in t:
            if section != "exit":
                print("[lesson4] --> Section: Exit check & Homework")
            section = "exit"
        elif "explanation" in t or "examples" in t:
            if section != "expl":
                print("[lesson4] --> Section: Explanation/Examples")
            section = "expl"

        # –ï—Å–ª–∏ —ç—Ç–æ –ø—É–Ω–∫—Ç —Å–ª–æ–≤–∞—Ä—è ‚Äî –¥–æ–±–∞–≤–∏–º TH
        if section == "vocab" and is_vocab_item(text):
            before = new_p.text
            # —Ñ—É–Ω–∫—Ü–∏—è –≤–µ—Ä–Ω—ë—Ç, –¥–æ–±–∞–≤–ª—è–ª–∏ –ª–∏ RU/TH
            ru_added = False
            th_added = False
            if args.vocab_th:
                _, th_added = append_th_to_vocab_line(new_p)
            # –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å RU –∫ —Å–ª–æ–≤–∞—Ä—é, –ø–æ–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–∫–ª—é—á–µ–Ω–æ
            if args.vocab_ru and not ru_added:
                # RU –æ–±—ã—á–Ω–æ —É–∂–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –ª–µ–∫—Å–∏–∫–µ, –ø–æ—ç—Ç–æ–º—É –∑–¥–µ—Å—å –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                pass
            if ru_added:
                vocab_ru_added += 1
            if th_added:
                vocab_th_added += 1
            continue  # –¥–ª—è —Å–ª–æ–≤–∞—Ä—è RU-—Å—Ç—Ä–æ–∫—É –æ—Ç–¥–µ–ª—å–Ω—É—é –Ω–µ –≤—Å—Ç–∞–≤–ª—è–µ–º (–æ–Ω–∞ —É–∂–µ –Ω–∞ –ª–∏–Ω–∏–∏)

        # –î–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö EN —Å—Ç—Ä–æ–∫ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º RU —Å—Ç—Ä–æ–∫—É
        stripped = text.strip()
        if not stripped:
            continue
        # –î–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Ä–∞–∑–¥–µ–ª–æ–≤ –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º (–Ω–µ –ø–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞–∑–≤–∞–Ω–∏—è –±–ª–æ–∫–æ–≤)
        if stripped in BLOCK_TITLES:
            continue

        # –ò—â–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã –≤ —Å–ª–æ–≤–∞—Ä–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ –∞–≤—Ç–æ-–ø–µ—Ä–µ–≤–æ–¥–∞). –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ —Ñ–ª–∞–≥–∞–º.
        key = norm_exact(text)
        has_any = False
        if args.with_ru:
            ru_txt = tr_map.get(key, {}).get("ru")
            if ru_txt:
                add_ru_mapped_line_with_highlights(out, p, ru_txt)
                ru_lines += 1
                has_any = True
            else:
                # –∫–æ—Ä–æ—Ç–∫–∏–π –ª–æ–≥ –ø–æ –ø—Ä–æ–ø—É—Å–∫–∞–º RU
                print(f"[lesson4][miss][RU] {key[:80]}")
        if args.with_th:
            th_txt = tr_map.get(key, {}).get("th")
            if th_txt:
                add_th_mapped_line_with_highlights(out, p, th_txt)
                has_any = True
            else:
                print(f"[lesson4][miss][TH] {key[:80]}")
        # –ü–µ—Ä–µ–≤–æ–¥ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ; –∞–≤—Ç–æ-–ø–µ—Ä–µ–≤–æ–¥ –æ—Ç–∫–ª—é—á—ë–Ω

        # –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 20 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        if idx % 20 == 0:
            print(f"[lesson4] Progress: {idx}/{total} paragraphs processed")

    out.save(OUT_NAME)
    dur = time.time() - start_ts
    print("[lesson4] Saved:", OUT_NAME)
    print(
        f"[lesson4] Summary: RU lines added={ru_lines}, vocab TH added={vocab_th_added}, vocab RU added={vocab_ru_added}")
    print(f"[lesson4] Done in {dur:.1f}s")


if __name__ == "__main__":
    build()
